{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54843e58",
   "metadata": {},
   "source": [
    "# Oracle 26ai를 Vector Store로 사용하여 Python/LangChain에서 RAG 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326294a",
   "metadata": {},
   "source": [
    "## Step 1: Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd510903",
   "metadata": {},
   "source": [
    "### Step 1.1: Loading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42362140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import re\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import SeleniumURLLoader\n",
    "\n",
    "urls = [\"https://www.busan.com/view/busan/view.php?code=2025103023015638180\",\n",
    "       \"https://www.yna.co.kr/view/AKR20251031055500003\",\n",
    "       \"https://biz.sbs.co.kr/article/20000270136\"]   \n",
    "\n",
    "# 뉴스기사를 로드합니다\n",
    "loader = WebBaseLoader(urls)\n",
    "docs = loader.load()\n",
    "\n",
    "for doc in docs:\n",
    "    url = doc.metadata['source']\n",
    "    html = requests.get(url).text\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # 메타데이타에 사이트이름을 추가합니다.\n",
    "    meta_tag = soup.find(\"meta\", attrs={\"property\": \"og:site_name\"}) or soup.find(\"meta\", attrs={\"name\": \"nate:site_name\"})\n",
    "    site_name = meta_tag[\"content\"] if meta_tag and meta_tag.get(\"content\") else None\n",
    "    doc.metadata[\"site_name\"] = site_name\n",
    "\n",
    "    # 로딩한 텍스트를 정제합니다.\n",
    "    clean_text = re.sub(r'\\n\\s*\\n+', '\\n', doc.page_content)  # 빈 줄 제거\n",
    "    clean_text = re.sub(r'[ \\t]+', ' ', clean_text)           # 중복 공백 제거\n",
    "    clean_text = clean_text.strip()\n",
    "    doc.page_content = clean_text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da783cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(f\"metadata: {doc.metadata}\")\n",
    "    print(f\"page_content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8f53a",
   "metadata": {},
   "source": [
    "### Step 1.2: Splitting documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ccdcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,       # chunk size (characters)\n",
    "    chunk_overlap=50,     # chunk overlap (characters)\n",
    "    add_start_index=True, # track index in original document\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    keep_separator=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"텍스트 분할: {len(docs)}개 문서가 {len(all_splits)}개로 분할됨\")\n",
    "\n",
    "for index, split in enumerate(all_splits):\n",
    "    print(f\"#### index: {index}\")\n",
    "    print(f\"     split: {split}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0077f",
   "metadata": {},
   "source": [
    "### Step 1.3: Storing documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c917128",
   "metadata": {},
   "source": [
    "#### Text embedding\n",
    "\n",
    "LangChain 문서 다음 링크에서 여러 embedding 모델에 따른 예시를 확인할 수 있습니다.\n",
    "- https://python.langchain.com/docs/how_to/embed_text/#setup\n",
    "\n",
    "여기서는 OCI Generative AI를 사용합니다.\n",
    "\n",
    "자신이 사용하는 Compartment ID를 확인합니다.\n",
    "\n",
    "-> [OCI Console에서 확인하러 가기](https://cloud.oracle.com/identity/compartments)\n",
    "\n",
    "**확인 후 [.env](.env) 파일에 업데이트합니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa03b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "compartment_id = os.getenv(\"COMPARTMENT_ID\")\n",
    "print(f\"compartment_id: {compartment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a00d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_oci import OCIGenAIEmbeddings\n",
    "\n",
    "# cohere.embed-v4.0 모델로 임베딩합니다.\n",
    "embeddings_model = OCIGenAIEmbeddings(\n",
    "    model_id=\"cohere.embed-v4.0\",\n",
    "    service_endpoint=\"https://inference.generativeai.ap-osaka-1.oci.oraclecloud.com\",\n",
    "    compartment_id=compartment_id,\n",
    "    auth_type=\"INSTANCE_PRINCIPAL\"\n",
    ")\n",
    "\n",
    "query_text = \"젠슨황이 2025년 10월에 이재용, 정의선과 치맥한 가게 이름은?\"\n",
    "query_vector = embeddings_model.embed_query(query_text)\n",
    "\n",
    "print(f\"query_text: {query_text}\")\n",
    "print(f\"query_vector: {query_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69559090",
   "metadata": {},
   "source": [
    "#### VectorStore: OracleVS vector store\n",
    "\n",
    "LangChain 문서 다음 링크에서 여러 vector store에 따른 예시를 확인할 수 있습니다.\n",
    "- https://python.langchain.com/docs/how_to/embed_text/#setup\n",
    "- https://python.langchain.com/docs/integrations/vectorstores/\n",
    "\n",
    "여기서는 Oracle Database 26ai를 vector store로 사용합니다.\n",
    "- https://python.langchain.com/docs/integrations/vectorstores/oracle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Container Image 사용하는 경우\n",
    "import oracledb\n",
    "\n",
    "username = \"vector\"\n",
    "password = \"vector\"\n",
    "dsn = \"localhost:1521/FREEPDB1\"\n",
    "\n",
    "try:\n",
    "    connection = oracledb.connect(user=username, password=password, dsn=dsn)\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.callproc(\"DBMS_APPLICATION_INFO.SET_CLIENT_INFO\", (\"oracle-devday/langchain-lab\",))\n",
    "\n",
    "    print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(\"Connection failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b8e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCI Autonomous AI Databases 사용하는 경우\n",
    "# password, dsn을 업데이트합니다.\n",
    "import oracledb\n",
    "\n",
    "username = \"vector\"\n",
    "password = \"OraclePass123\"\n",
    "dsn = \"yo64vsjrusqy4hks_low\"\n",
    "\n",
    "try:\n",
    "    connection = oracledb.connect(\n",
    "        config_dir=\"../wallet\",\n",
    "        user=username,\n",
    "        password=password,\n",
    "        dsn=dsn,\n",
    "        wallet_location=\"../wallet\",\n",
    "        wallet_password=password)\n",
    "    \n",
    "    print(\"Oracle 26ai Autonomous Database Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(\"Oracle 26ai Autonomous Database Connection failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_oracledb.vectorstores import OracleVS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "vector_store = OracleVS(\n",
    "    client=connection,        \n",
    "    table_name=\"DOCUMENTS_COSINE\",\n",
    "    embedding_function=embeddings_model,\n",
    "    distance_strategy=DistanceStrategy.COSINE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eaa0c8",
   "metadata": {},
   "source": [
    "**SQL Worksheet**로 DB에 접속하여 다음을 실행합니다. (**SQLcl**은 Python Environment와 충돌하여, 동작하지 않을 수 있습니다.)\n",
    "\n",
    "```sql\n",
    "desc DOCUMENTS_COSINE;\n",
    "```\n",
    "\n",
    "-> [SQL Worksheet로 이동하기](26ai-vector-search-langchain-lab.sql:7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c46d5",
   "metadata": {},
   "source": [
    "#### Storing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf29259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할된 청크들을 Vector Store에 저장합니다.\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80429e8",
   "metadata": {},
   "source": [
    "**SQL Worksheet**로 DB에 접속하여 다음을 실행합니다.\n",
    "\n",
    "```sql\n",
    "SELECT id, text, to_char(METADATA), EMBEDDING FROM DOCUMENTS_COSINE;\n",
    "```\n",
    "\n",
    "-> [SQL Worksheet로 이동하기](26ai-vector-search-langchain-lab.sql:10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3081a",
   "metadata": {},
   "source": [
    "## Step 2: Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3335d242",
   "metadata": {},
   "source": [
    "### Step 2.1: Retrieve Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"젠슨황이 2025년 10월에 이재용, 정의선과 치맥한 가게 이름은?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffdbcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b4e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(user_question)\n",
    "print(len(retrieved_docs))\n",
    "\n",
    "import json\n",
    "\n",
    "for index, doc in enumerate(retrieved_docs):\n",
    "    print(f\"#### index: {index}\")\n",
    "    print(f\"     metadata: {doc.metadata}\")\n",
    "    print(f\"     page_content: {doc.page_content[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f7fc3a",
   "metadata": {},
   "source": [
    "### Step 2.1-1: Retrieve with Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "from typing import List\n",
    "\n",
    "@chain\n",
    "def retriever(query: str) -> List[Document]:\n",
    "    docs, scores = zip(*vector_store.similarity_search_with_score(query))\n",
    "    for doc, score in zip(docs, scores):\n",
    "        doc.metadata[\"score\"] = score\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(user_question)\n",
    "print(len(retrieved_docs))\n",
    "\n",
    "import json\n",
    "\n",
    "for index, doc in enumerate(retrieved_docs):\n",
    "    print(f\"#### index: {index}\")\n",
    "    print(f\"     metadata: {doc.metadata}\")\n",
    "    print(f\"     page_content: {doc.page_content}\")\n",
    "    print(f\"     score: {doc.metadata['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df6246a",
   "metadata": {},
   "source": [
    "### Step 2.2: Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf166f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Respond in Korean.\n",
    "Always include the source of your answer, specifying the original document's title and link, in the format [출처: 문서 제목, 링크].\n",
    "\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba498a8",
   "metadata": {},
   "source": [
    "### Step 2.3: LLM\n",
    "\n",
    "LangChain 문서 다음 링크에서 여러 chat model에 따른 예시를 확인할 수 있습니다.\n",
    "- https://python.langchain.com/docs/integrations/chat/\n",
    "\n",
    "여기서는 로컬 테스트를 위해 Ollama를 사용합니다.\n",
    "- https://python.langchain.com/docs/integrations/chat/ollama/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5e2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 LLM 사용\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    #model=\"llama3.1\",         #llama3.1:8b\n",
    "    #model=\"llama3.2:latest\",  #llama3.2:3b\n",
    "    model=\"llama3.2:1b\",\n",
    "    temperature=0,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c607d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCI Generative AI 사용\n",
    "from langchain_oci import ChatOCIGenAI\n",
    "\n",
    "llm = ChatOCIGenAI(\n",
    "    #model_id=\"meta.llama-3.2-90b-vision-instruct\",\n",
    "    model_id=\"openai.gpt-oss-120b\",\n",
    "    service_endpoint=\"https://inference.generativeai.ap-osaka-1.oci.oraclecloud.com\",\n",
    "    compartment_id=compartment_id,\n",
    "    model_kwargs={\"temperature\": 0},\n",
    "    auth_type=\"INSTANCE_PRINCIPAL\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44464440",
   "metadata": {},
   "source": [
    "### Step 2.4: Answer\n",
    "llama3.1:8b: AMD 1 OCPU - 6m 23s\n",
    "llama3.2:1b: AMD 1 OCPU - 1m 2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f70142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "response = llm.invoke(user_question)\n",
    "print(f\"# LLM에 직접 질문하기\")\n",
    "print(f\"# user_question: {user_question}\")\n",
    "print(f\"# response: {response.content}\")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = user_question\n",
    "response = chain.invoke(question)\n",
    "\n",
    "print(f\"\\n# RAG를 사용하여 질문하기\")\n",
    "print(f\"# user_question: {user_question}\")\n",
    "print(f\"# response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb569396",
   "metadata": {},
   "source": [
    "## Step 3: OracleVS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e444ec3",
   "metadata": {},
   "source": [
    "**SQL Worksheet**로 DB에 접속하여 다음을 실행합니다.\n",
    "앞 코드 실행으로 인해 DB에서 실행한 SQL 쿼리를 확인하는 질의입니다.\n",
    "\n",
    "```sql\n",
    "SELECT sql_id, parsing_schema_name, sql_text\n",
    "FROM v$sql\n",
    "WHERE parsing_schema_name = 'VECTOR' and module like '%python'\n",
    "ORDER BY last_active_time DESC;\n",
    "````\n",
    "\n",
    "-> [SQL Worksheet로 이동하기](26ai-vector-search-langchain-lab.sql:19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b9eb0",
   "metadata": {},
   "source": [
    "실행한 SQL 쿼리에 대한 실행 계획을 확인합니다.\n",
    "\n",
    "- 예시\n",
    "```sql\n",
    "EXPLAIN PLAN FOR\n",
    "    SELECT id, text, metadata, vector_distance(embedding, :embedding, COSINE) as distance\n",
    "      FROM DOCUMENTS_COSINE\n",
    "  ORDER BY distance\n",
    "     FETCH APPROX FIRST 4 ROWS ONLY;\n",
    "\n",
    "SELECT * FROM TABLE(DBMS_XPLAN.DISPLAY(format => 'ALL'));\n",
    "```\n",
    "\n",
    "-> [SQL Worksheet로 이동하기](26ai-vector-search-langchain-lab.sql:25)\n",
    "\n",
    "생성된 테이블에 INDEX가 없으므로, FULL Search하는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94791d45",
   "metadata": {},
   "source": [
    "### Step 3.1: Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37dccb",
   "metadata": {},
   "source": [
    "#### Step 3.1.1: HNSW Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_oracledb.vectorstores import oraclevs\n",
    "\n",
    "oraclevs.create_index(\n",
    "    connection,\n",
    "    vector_store,\n",
    "    params={\"idx_name\": \"documents_cosine_hnsw_idx\", \"idx_type\": \"HNSW\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085dc77",
   "metadata": {},
   "source": [
    "**SQL Worksheet**로 DB에 접속하여 다음을 실행합니다.\n",
    "\n",
    "```sql\n",
    "SELECT * FROM ALL_INDEXES WHERE table_name=UPPER('DOCUMENTS_COSINE');\n",
    "```\n",
    "\n",
    "인덱스가 생성된 것을 확인합니다.\n",
    "\n",
    "-> [SQL Worksheet로 이동하기](26ai-vector-search-langchain-lab.sql:34)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75a6ad",
   "metadata": {},
   "source": [
    "Step 2.4과 동일한 내용으로 LLM에 다시 요청합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9870a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(question)\n",
    "\n",
    "print(f\"# user_question: {user_question}\")\n",
    "print(f\"# response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef2fe84",
   "metadata": {},
   "source": [
    "**SQL Worksheet**로 DB에 접속하여 다음을 실행합니다.\n",
    "실행 계획이 달라졌는 지 확인합니다.\n",
    "\n",
    "- 예시\n",
    "```sql\n",
    "EXPLAIN PLAN FOR\n",
    "    SELECT id, text, metadata, vector_distance(embedding, :embedding, COSINE) as distance\n",
    "      FROM DOCUMENTS_COSINE\n",
    "  ORDER BY distance\n",
    "     FETCH APPROX FIRST 4 ROWS ONLY;\n",
    "\n",
    "SELECT * FROM TABLE(DBMS_XPLAN.DISPLAY(format => 'ALL'));\n",
    "```\n",
    "\n",
    "-> [SQL Worksheet로 이동하기](26ai-vector-search-langchain-lab.sql:37)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c751e",
   "metadata": {},
   "source": [
    "#### Step 3.1.2: IVF Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92626069",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 앞서 생성한 HNSW 인덱스를 삭제합니다.\n",
    "from langchain_oracledb.vectorstores import oraclevs\n",
    "\n",
    "oraclevs.drop_index_if_exists(\n",
    "    connection,\n",
    "    index_name=\"documents_cosine_hnsw_idx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838dcd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_oracledb.vectorstores import oraclevs\n",
    "\n",
    "oraclevs.create_index(\n",
    "    connection,\n",
    "    vector_store,\n",
    "    params={\"idx_name\": \"documents_cosine_ivf_idx\", \"idx_type\": \"IVF\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0e400",
   "metadata": {},
   "source": [
    "### Step 3.2: Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09ce53",
   "metadata": {},
   "source": [
    "#### Step 3.2.1: Filtering - filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f6aac",
   "metadata": {},
   "source": [
    "필터링 없이 유사도 검색은 다음과 같이 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a227dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity search without a filter\n",
    "print(\"\\nSimilarity search results without filter:\")\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(question, 2)\n",
    "\n",
    "for index, doc in enumerate(retrieved_docs):\n",
    "    print(f\"#### index: {index}\")\n",
    "    print(f\"     metadata: {doc.metadata}\")\n",
    "    print(f\"     page_content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1b7259",
   "metadata": {},
   "source": [
    "similarity_search() 함수를 통해 문서 retrieved_docs를 응답으로 받는 경우, chain 말고 각 문서의 page_content의 문자열을 context로 직접 사용하는 것도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3406675",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "question = user_question\n",
    "\n",
    "prompt = f\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Respond in Korean.\n",
    "\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(f\"# user_question: {user_question}\")\n",
    "print(f\"# response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851692a6",
   "metadata": {},
   "source": [
    "metadata 칼럼 기준으로 아래와 같이 필터링 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e10dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity search with a filter\n",
    "print(\"\\nSimilarity search results with filter:\")\n",
    "\n",
    "#filter_dict = {\"source\": \"https://www.busan.com/view/busan/view.php?code=2025103023015638180\"};\n",
    "filter_dict = {\"site_name\": \"부산일보\"};\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(question, 3, filter=filter_dict);\n",
    "\n",
    "for index, doc in enumerate(retrieved_docs):\n",
    "    print(f\"#### index: {index}\")\n",
    "    print(f\"     metadata: {doc.metadata}\")\n",
    "    print(f\"     page_content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f9d96",
   "metadata": {},
   "source": [
    "**SQL Worksheet**로 DB에 접속하여 다음을 실행합니다.\n",
    "\n",
    "실행된 쿼리를 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e81c78",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT sql_id, parsing_schema_name, sql_text\n",
    "FROM v$sql\n",
    "WHERE parsing_schema_name = 'VECTOR' and module like '%python' and sql_text like '%DOCUMENTS_COSINE%'\n",
    "ORDER BY last_active_time DESC;\n",
    "```\n",
    "\n",
    "-> [SQL Worksheet로 이동하기](26ai-vector-search-langchain-lab.sql:46)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def90e28",
   "metadata": {},
   "source": [
    "실행된 쿼리를 확인해 보면 필터링이 **WHERE JSON_EXISTS**로 **Where 절 조건**이 추가된 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d49c8",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT id, text, metadata, vector_distance(embedding, :embedding, COSINE) as distance\n",
    "FROM \"DOCUMENTS_COSINE\"\n",
    "WHERE JSON_EXISTS(metadata, '$.source?(@ == $val)' PASSING :value0 AS \"val\")\n",
    "ORDER BY distance\n",
    "FETCH APPROX FIRST 3 ROWS ONLY\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808b3d36",
   "metadata": {},
   "source": [
    "JSON 타입인 metadata에 대해서 추가적인 인덱스 설정이 필요합니다.\n",
    "\n",
    "**SQL Worksheet**로 DB에 접속하여 다음을 실행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638cacf",
   "metadata": {},
   "source": [
    "```sql\n",
    "CREATE SEARCH INDEX metadata_json_search_idx\n",
    "ON DOCUMENTS_COSINE (metadata)\n",
    "FOR JSON;\n",
    "```\n",
    "\n",
    "-> [SQL Worksheet로 이동하기](26ai-vector-search-langchain-lab.sql:52)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79d740",
   "metadata": {},
   "source": [
    "인덱스 생성후 다시 확인해 보면 새로운 JSON 인덱스를 사용하는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6a094",
   "metadata": {},
   "source": [
    "```sql\n",
    "EXPLAIN PLAN FOR\n",
    "  SELECT id, text, metadata, vector_distance(embedding, :embedding, COSINE) as distance\n",
    "    FROM \"DOCUMENTS_COSINE\"\n",
    "   WHERE JSON_EXISTS(metadata, '$.source?(@ == $val)' PASSING :value0 AS \"val\")\n",
    "ORDER BY distance\n",
    "   FETCH APPROX FIRST 3 ROWS ONLY;\n",
    "\n",
    "SELECT * FROM TABLE(DBMS_XPLAN.DISPLAY(format => 'ALL'));\n",
    "```\n",
    "\n",
    "-> [SQL Worksheet로 이동하기](26ai-vector-search-langchain-lab.sql:57)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdbb0b1",
   "metadata": {},
   "source": [
    "#### Step 3.2.2: Filtering - filter : score 함께 가져오기 (참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2205ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity search with relevance score\n",
    "print(\"\\nSimilarity search with relevance score:\")\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search_with_score(question, 2);\n",
    "\n",
    "for index, doc_with_score in enumerate(retrieved_docs):\n",
    "    print(f\"#### index: {index}\")\n",
    "    #print(type(doc_with_score))\n",
    "    doc = doc_with_score[0]\n",
    "    score = doc_with_score[1]\n",
    "    print(f\"     metadata: {doc.metadata}\")\n",
    "    print(f\"     page_content: {doc.page_content}\")\n",
    "    print(f\"     score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d530037",
   "metadata": {},
   "source": [
    "## Step 4: Existing Table with VECTOR type\n",
    "이전 내용들은 LangChain에서 문서를 읽어 분리하고, 임베딩하고 DB 테이블에 저장하는 일련의 인덱싱 작업 후에 조회하는 과정이었습니다. OracleVS 패키지 또한 LangChain 연동에 맞춰 구현된 내용으로 그에 맞게 테이블 구조가 자동으로 만들어 진 상태였습니다.\n",
    "\n",
    "여기서는 사용자가 이미 있는 테이블, 데이터에 대해 Oracle AI Database 26ai Vector Search 기능을 통해 유사도 질의를 하는 경우, LangChain에서 어떻게 사용할 까 하는 부분입니다. oracledb 패키지를 통해 Python에서 SQL 질의하고, 그 결과를 LangChain에 사용하는 langchain_core.documents.base.Document 클래스 형태로 변경하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e585c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "#embeddings_model = OllamaEmbeddings(model=\"paraphrase-multilingual\")\n",
    "\n",
    "from langchain_oci import ChatOCIGenAI\n",
    "\n",
    "# cohere.embed-v4.0 모델로 임베딩합니다.\n",
    "embeddings_model = OCIGenAIEmbeddings(\n",
    "    model_id=\"cohere.embed-v4.0\",\n",
    "    service_endpoint=\"https://inference.generativeai.ap-osaka-1.oci.oraclecloud.com\",\n",
    "    compartment_id=compartment_id,\n",
    "    auth_type=\"INSTANCE_PRINCIPAL\"\n",
    ")\n",
    "\n",
    "query_vector = embeddings_model.embed_query(\"다국어 메뉴판이 있는 여의도 맛집\")\n",
    "    \n",
    "#print(type(query_vector)) # <class 'list'>\n",
    "query_vector_str = str(query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5bc1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "cursor = connection.cursor()    \n",
    "\n",
    "query = \"\"\"\n",
    "SELECT id, VECTOR_DISTANCE(VECTOR_DESCRIPTION, :query_vector, COSINE) AS vector_distance, business_type_registered, name, description\n",
    "FROM RSTR_INFO\n",
    "WHERE business_type_registered='한식'\n",
    "ORDER BY vector_distance\n",
    "FETCH FIRST 10 ROWS ONLY\n",
    "\"\"\"\n",
    "cursor.execute(query, {\"query_vector\": query_vector_str})\n",
    "results = cursor.fetchall()\n",
    "\n",
    "retrieved_docs = []\n",
    "\n",
    "for result in results:\n",
    "    metadata = {\n",
    "        \"id\": result[0],\n",
    "        \"business_type_registered\": result[2],\n",
    "        \"name\": result[3]\n",
    "        }\n",
    "            \n",
    "    doc = Document(\n",
    "        page_content=(\n",
    "            result[4]\n",
    "            if result[4] is not None\n",
    "            else \"\"\n",
    "        ),\n",
    "        metadata=metadata,\n",
    "    )\n",
    "\n",
    "    retrieved_docs.append(doc)\n",
    "\n",
    "for index, doc in enumerate(retrieved_docs):\n",
    "    print(f\"#### index: {index}\")\n",
    "    print(f\"     metadata: {doc.metadata}\")\n",
    "    print(f\"     page_content: {doc.page_content}\")\n",
    "\n",
    "print(\"\\n\".join([f\"{doc.metadata['name']}: {doc.page_content}\" for doc in retrieved_docs]))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
